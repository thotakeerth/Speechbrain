"""
Data preparation.
Assume that transcription (.stm) is generated by Transcriber export.

Download (paid) ESTER1: https://catalogue.elra.info/en-us/repository/browse/ELRA-S0241/
Download (paid) ESTER2: https://catalogue.elra.info/en-us/repository/browse/ELRA-S0338/
Download (paid) ETAPE:  https://catalogue.elra.info/en-us/repository/browse/ELRA-E0046/
Download (paid) EPAC:   https://catalogue.elra.info/en-us/repository/browse/ELRA-S0305/
Download (paid) REPERE: https://catalogue.elra.info/en-us/repository/browse/ELRA-E0044/

Author
------
Pierre Champion
"""

import csv
import glob
import logging
import os
import re
import string
import sys

import ftfy
import soundfile
from num2words import num2words
from tqdm import tqdm

from speechbrain.dataio.dataio import load_pkl, save_pkl
from speechbrain.utils.data_utils import get_list_from_csv

logger = logging.getLogger(__name__)
OPT_FILE = "opt_stm_prepare.pkl"
SAMPLERATE = 16000


logging.basicConfig(level=logging.INFO)


def prepare_stm(  # noqa
    stm_directory,
    wav_directory,
    tr_splits,
    dev_splits,
    te_splits,
    save_folder,
    merge_train_csv,
    train_csv,
    skip_prep=False,
    ignore_wav=False,
    new_word_on_apostrophe=True,
):
    """
    This class prepares the csv files for STM like dataset.

    Arguments
    ---------
    stm_directory : str
        Path to the folder where the original .stm files are stored (glob compatible)
    wav_directory : str
        Path to the folder where the original .wav files are stored (glob compatible)
    tr_splits : Union[list, Dict]
        List of train splits (regex from path) to prepare from ["/train*", "other_data"]
        Dict of List of train splits (regex from path) to prepare from {"train_ETAPE":["/ETAPE/train/*"], "train_ESTER2":["/ESTER2/train/*"], "train_ESTER1":["/ESTER1/train/*"]},
    dev_splits : Union[list, Dict]
        List of dev splits (regex from path) to prepare from ["*/dev/*"].
        Dict of List of test splits (regex from path) to prepare from dev_splits: {"dev_ESTER2":["/ESTER2/dev/*"], "dev_ESTER1":["/ESTER1/dev/*"], "dev_ETAPE":["/ETAPE/dev/*"], "dev_REPERE2014":["/REPERE/dev2014/*"]}
    te_splits : Union[list, Dict]
        List of test splits (regex from path) to prepare from ["*/test/*"] -> create one test merged test dataset.
        Dict of List of test splits (regex from path) to prepare from {"test_ETAPE":["/ETAPE/test/*"], "test_ESTER2":["/ESTER2/test/*"], "test_ESTER1":["/ESTER1/test/*"]},
    save_folder : str
        The directory where to store the csv files.
    merge_train_csv : List[str]
        List of tr_splits to concatenate in train_csv
    train_csv: str
        Where to store the final train csv from the splits defined in merge_train_csv
    skip_prep: bool
        If True, data preparation is skipped.

    Example
    -------
    >>> prepare_stm(
    ...     "/corpus/**/[^\.ne_e2\.|\.ne\.|\.spk\.|part\.]*.stm",  # noqa
    ...     "/corpus/**/*.wav",
    ...     [r"/train/", r"/train_trans_rapide/"],
    ...     [r"/dev/"],
    ...     [r"/test/"],
    ...     "./data_prep_out",
    ...)

    >>> prepare_stm(
    ...     "/corpus/ESTER[1-2]/**/[^\.ne_e2\.|\.ne\.|\.spk\.]*.stm",  # noqa
    ...     "/corpns/ESTER[1-2]/**/*.wav",
    ...     [r"/train/", r"/train_trans_rapide/"],
    ...     [r"/dev/"],
    ...     [r"/test/"],
    ...     "./data_prep_out",
    ...)

    >>> prepare_stm(
    ...     "/corpns/ESTER2/**/[^\.ne_e2\.|\.ne\.|\.spk\.]*.stm",  # noqa
    ...     "/corpus/ESTER2/**/*.wav",
    ...     [r"/train/", r"/train_trans_rapide/"],
    ...     [r"/dev/"],
    ...     [r"/test/"],
    ...     "./data_prep_out",
    ...)

    """

    if skip_prep:
        return

    os.makedirs(save_folder, exist_ok=True)

    conf = locals().copy()
    save_opt = os.path.join(save_folder, OPT_FILE)
    # Check if this phase is already done (if so, skip it)
    if skip(save_folder, conf, save_opt):
        logger.info("Skipping preparation, completed in previous run.")
        return
    else:
        logger.info("Data_preparation...")

    stm_paths, stm_exclude_match = custom_glob_filter(stm_directory)
    pbar = tqdm(stm_paths, bar_format="{desc} {percentage:3.0f}%")
    i = 0

    if not ignore_wav:
        wav_paths, wav_exclude_match = custom_glob_filter(wav_directory)
        wav_paths_map = {
            normalize_wav_key(os.path.basename(wav_p)): wav_p
            for wav_p in wav_paths
        }

    split_info = []
    for split, default_name in zip(
        [tr_splits, dev_splits, te_splits], ["train", "dev", "test"]
    ):
        if isinstance(split, list):
            split_info.append((split, default_name, []))
        if isinstance(split, dict):
            for te_splits_name, te_splits_paths in split.items():
                split_info.append((te_splits_paths, te_splits_name, []))

    train_vocab = set()
    train_transcript_words = []

    for filename in pbar:
        if (
            stm_exclude_match is not None
        ):  # Exclude all paths with specified string
            if re.search(stm_exclude_match, filename):
                logger.debug(
                    f"Skipping {filename}, as it is in the exclude match"
                )
                continue

        split = "n"
        info = None
        for sp, id, _info in split_info:
            for tr in sp:
                if re.search(tr, filename):
                    split = id
                    info = _info

        if split == "n":
            logger.debug(f"Skipping {filename}, not associated to any split")
            continue

        d = (filename).ljust(80, " ")
        i += 1
        pbar.set_description(
            f"Len: {str(i).rjust(4)} : Processing '{split}' : {d}"
        )
        with open(filename, "r") as file:
            data = file.readlines()
        for line in norm_stm_lines(filterout_lines(data)):
            parts = line.split()
            wav_key = normalize_wav_key(parts[0])
            if not ignore_wav and wav_key not in wav_paths_map:
                logger.critical(
                    f"Did not found wav '{wav_key}' for stm: '{filename}'"
                )
                break
            else:
                text = normalize_text(
                    f"{' '.join(parts[6:])}",
                    new_word_on_apostrophe=new_word_on_apostrophe,
                )

                # No transcription, might be only rire/jingle annotation
                if text == "":
                    continue

                if not ignore_wav:
                    # wav file is not complete
                    audio_info = soundfile.info(wav_paths_map[wav_key])
                    startTime = float(parts[3])
                    endTime = float(parts[4])
                    wav_path = wav_paths_map[wav_key]
                    if (
                        startTime > audio_info.duration
                        or int(endTime) > audio_info.duration
                    ):
                        logger.critical(
                            f"Skipping, segment StartTime or endTime ({startTime},{endTime}) longer than wav file ({audio_info.duration})"
                        )
                        continue
                else:
                    # Text only
                    startTime = 0.0
                    endTime = 1.0
                    wav_path = "/dev/null"

                if split == "train":
                    train_transcript_words.append(text)
                    for word in set(text.split(" ")):
                        train_vocab.add(word)

                info.append(
                    {
                        "ID": f"{parts[0]}-{parts[3].replace('.', '')[:-1].zfill(7)}-{parts[4].replace('.', '')[:-1].zfill(7)}",
                        "wrd": text,
                        "spk": parts[2],
                        "gender": f"{parts[5].split(',', 3)[2].replace('>', '')}",
                        "startTime": startTime,
                        "endTime": endTime,
                        "duration": endTime - startTime,
                        "file": wav_path,
                    }
                )

    merge_train_csv = [m.strip() for m in merge_train_csv]
    with open(train_csv, "w") as merge_csvfile:
        merge_writer = None

        for path, split, info in split_info:

            # Sort the data based on column ID
            sorted_formatted_data = sorted(info, key=lambda x: x["ID"])

            if len(sorted_formatted_data) == 0:
                logger.critical(
                    f"No file found for {split} {path} check directory paths."
                )
                continue

            csv_file = os.path.join(save_folder, split + ".csv")
            with open(csv_file, "w") as csvfile:
                writer = csv.DictWriter(
                    csvfile, fieldnames=sorted_formatted_data[0].keys()
                )
                writer.writeheader()
                writer.writerows(sorted_formatted_data)

            if split in merge_train_csv:
                if not merge_writer:
                    merge_writer = csv.DictWriter(
                        merge_csvfile,
                        fieldnames=sorted_formatted_data[0].keys(),
                    )
                    merge_writer.writeheader()
                merge_writer.writerows(sorted_formatted_data)

    sorted_vocabulary = sorted(train_vocab)
    vocab_file = os.path.join(save_folder, "vocab.txt")
    with open(vocab_file, "w") as file:
        for word in sorted_vocabulary:
            if word == " ":
                continue
            if word == "":
                continue
            # Write each word to the file, followed by a newline character
            file.write(word + "\n")

    transcript_words = os.path.join(save_folder, "transcript_words.txt")
    with open(transcript_words, "w") as file:
        for line in train_transcript_words:
            file.write(line + "\n")

    # saving options
    save_pkl(conf, save_opt)


def normalize_wav_key(key):
    """
    This function normalizes a key typically used for referencing WAV files.

    Arguments
    ---------
    key : str
        The original key to be normalized.

    Returns
    ---------
    str
        The normalized key after removing specific substrings, replacing characters, and converting to lowercase.
    """
    key = key.replace("suite", "")
    key = key.replace("_bis", "")
    key = key.replace("automatique", "")
    key = key.replace("wav", "")
    key = key.replace("-", "_")
    key = key.replace(".", "")
    key = key.lower()
    return key


def custom_glob_filter(directory):
    """
    This function provides custom filtering functionality for file paths using glob patterns.

    Arguments
    ---------
    directory : str
        The directory path or glob pattern for file matching.

    Returns
    ---------
    tuple
        A tuple containing two elements:
            - A list of paths matching the provided glob pattern in the directory.
            - An optional string specifying an exact match to be excluded from the results.
    """
    # Support for exclude exact word
    # https://stackoverflow.com/questions/20638040/glob-exclude-pattern
    try:  # Try to parse exact match direction
        exclude_match = (
            re.findall(r"\[\^.*\]", directory)[0]
            .replace("[^", "")
            .replace("]", "")
        )
    except IndexError:
        exclude_match = None
    else:  # Remove custom directive
        directory = re.sub(r"\[\^.*\]", "", directory)
    paths = glob.glob(directory, recursive=True)
    return paths, exclude_match


def norm_stm_lines(line):
    """
    Normalizes stm line

    Arguments
    ---------
    line : str or list of str
        The input line or list of lines to be transformed.

    Returns
    ---------
    list of str
        The transformed lines after performing string replacements.
    """
    line = [re.sub(r"<F0_M>", "<o,f0,male>", line) for line in line]
    line = [re.sub(r"<F0_F>", "<o,f0,female>", line) for line in line]
    line = [re.sub(r"\([0-9]+\)", "", line) for line in line]
    line = [re.sub(r"<sil>", "", line) for line in line]
    line = [re.sub(r"\([^ ]*\)$", "", line) for line in line]
    return line


def normalize_text(text, new_word_on_apostrophe=True):
    """
    This function normalizes text by applying various transformations.

    Arguments
    ---------
    text : str
        The original text to be normalized.

    new_word_on_apostrophe : Optional[bool]
        If True, inserts a space after an apostrophe, creating a new word. Default is True.

    Returns
    ---------
    str
        The normalized text after applying multiple substitutions and transformations, including:
        - Fixing text encoding issues.
        - Substituting specific words or phrases.
        - Removing or replacing special characters.
        - Converting numbers to words in French.
        - Handling punctuation and special cases.
    """

    text = ftfy.fix_text(text)

    # Names
    text = re.sub(r"Franç§ois", "François", text)  # codespell:ignore
    text = re.sub(r"Schrà ¶der", "Schràder", text)

    text = re.sub(r"«", "", text)
    text = re.sub(r"»", "", text)

    text = re.sub(r"°c", "degré", text)
    text = re.sub(r"°C", "degré", text)
    text = re.sub(r"°", "degré", text)

    text = re.sub(r"²", "", text)

    # remove html tag
    text = re.sub(r"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});", " ", text)

    # Replace curly braces with square brackets
    text = text.replace("{", "[").replace("}", "]")

    text = re.sub(r"\.\.\.|\*|\[.*?\]", "", text.lower())

    delset = string.punctuation
    delset = delset.replace("'", "")
    delset = delset.replace("%", "")
    delset = delset.replace(",", "")

    remove_clear = "()=-"
    for char in remove_clear:
        delset = delset.replace(char, "")
    text = text.translate(str.maketrans("", "", delset))

    # Undecidable variant heard like on (n') en:
    text = re.sub(r"\(.+?\)", "", text)
    text = re.sub(r"\(\)", "", text)
    text = re.sub(r"(O.K.)", "ok", text)
    text = re.sub(r"(O.K)", "ok", text)

    text = re.sub(r"%", "pour cent", text)

    text = re.sub(r"=", "", text)
    text = re.sub(r"\(", "", text)
    text = re.sub(r"\)", "", text)

    # t 'avais
    text = re.sub(r"[ ]\'", "'", text)
    # t' avais
    text = re.sub(r"\'[ ]", "'", text)
    if new_word_on_apostrophe:
        text = re.sub(r"\'", "' ", text)

    # ' en debut de phrase
    text = re.sub(r"^'", "", text)

    # -) hesitation
    text = re.sub(r"-\)", "", text)

    # convert to french time 15h -> 15 heure
    match = re.match(r"(\d+)h", text)
    text = f"{match.group(1)} heure" if match else text

    num_list = re.findall(" \d+,\d+ | \d+,\d+$", text)  # noqa
    if len(num_list) > 0:
        for num in num_list:
            num_in_word = num2words(float(num.replace(",", ".")), lang="fr")
            text = text.replace(num, " " + str(num_in_word) + " ", 1)

    num_list = re.findall("\d+,\d+", text)  # noqa
    if len(num_list) > 0:
        for num in num_list:
            num_in_word = num2words(float(num.replace(",", ".")), lang="fr")
            text = text.replace(num, " " + str(num_in_word) + " ", 1)

    num_list = re.findall(" \d+ | \d+$", text)  # noqa
    if len(num_list) > 0:
        for num in num_list:
            num_in_word = num2words(int(num), lang="fr")
            text = text.replace(num, " " + str(num_in_word) + " ", 1)

    num_list = re.findall("\d+", text)  # noqa
    if len(num_list) > 0:
        for num in num_list:
            num_in_word = num2words(int(num), lang="fr")
            text = text.replace(num, " " + str(num_in_word) + " ", 1)

    # arc-en-ciel
    text = re.sub(r"-", " ", text)

    # virgule (after num2words!)
    text = re.sub(r",", "", text)

    # euh
    # text = re.sub(r"euh", "", text)

    # ã used as à in most case
    text = re.sub(r"ã", "à", text)

    # replace n successive spaces with one space.
    text = re.sub(r"\s{2,}", " ", text)
    text = re.sub("^ ", "", text)
    text = re.sub(" $", "", text)

    # The byte 0x9c encodes a "curly quote" in the Windows-1252 character encoding.
    text = re.sub(r"c½ur", "coeur", text)
    text = re.sub(r"cur", "coeur", text)
    # The byte 0x92 encodes a "curly quote" in the Windows-1252 character encoding.
    text = re.sub(r"", "'", text)
    text = re.sub(r"' '", "' ", text)
    text = re.sub(r"'' ", "' ", text)

    return text


def filterout_lines(lines):
    """
    This function filters out lines from stm entries based on specific patterns.

    Arguments
    ---------
    lines : list of str
        A list containing lines of text to be filtered.

    Returns
    ---------
    list of str
        A list containing lines of text after filtering out lines containing specific patterns such as "ignore_time_segment_in_scoring", ";;", "inter_segment_gap", and "excluded_region".

    """
    return [
        line
        for line in lines
        if not any(
            pattern in line
            for pattern in [
                "ignore_time_segment_in_scoring",
                ";;",
                "inter_segment_gap",
                "excluded_region",
            ]
        )
    ]


def dataprep_lm_training(
    lm_dir, output_arpa, csv_files, external_lm_corpus, vocab_file
):
    """Prepare lm txt corpus file for lm training with kenlm (https://github.com/kpu/kenlm)
    Does nothing if output_arpa exists.
    Else display to the user how to use kenlm in command line, then exit
    (return code 1), the user has to run the command manually.
    Instruction on how to compile kenlm (lmplz binary) is available in the
    above link.

    Arguments
    ---------
    lm_dir : str
        Path to where to store txt corpus
    output_arpa : str
        File to write arpa lm
    csv_files : List[str]
        CSV files to use to increase lm txt corpus
    external_lm_corpus : List[str]
        (Big) text dataset corpus
    vocab_file : str
       N-grams that contain vocabulary items not in this file be pruned.
    """
    if not os.path.exists(output_arpa):
        column_text_key = "wrd"
        lm_corpus = os.path.join(lm_dir, "lm_corpus.txt")
        line_seen = set()
        with open(lm_corpus, "w") as corpus:
            for file in csv_files:
                for line in get_list_from_csv(file, column_text_key):
                    corpus.write(line + "\n")
                    line_seen.add(line + "\n")
            for file in external_lm_corpus:
                with open(file) as f:
                    for line in f:
                        if line not in line_seen:
                            corpus.write(line)
        logger.critical(
            "RUN the following kenlm command to build a 3-gram arpa LM (https://github.com/kpu/kenlm):"
        )
        logger.critical(
            f"$ lmplz -o 3 --prune 0 1 2 --limit_vocab_file {vocab_file} < {lm_corpus}| sed  '1,20s/<unk>/<UNK>/1' > {output_arpa}"
        )
        sys.exit(1)


def skip(save_folder, conf, save_opt):
    """
    Detect when data prep can be skipped.

    Arguments
    ---------
    save_folder : str
        The location of the seave directory
    conf : dict
        The configuration options to ensure they haven't changed.

    Returns
    -------
    bool
        if True, the preparation phase can be skipped.
        if False, it must be done.
    """

    # Checking csv files
    skip = True

    if len(glob.glob(os.path.join(save_folder, "*.csv"), recursive=False)) == 0:
        logger.info(f"Did not found any csv in '{save_folder}'")
        skip = False
    else:
        logger.info(f"Found csv in '{save_folder}'")

    #  Checking saved options
    if skip is True:
        if os.path.isfile(save_opt):
            opts_old = load_pkl(save_opt)
            if opts_old == conf:
                skip = True
            else:
                skip = False
        else:
            skip = False

    return skip
